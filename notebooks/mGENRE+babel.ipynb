{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from genre.trie import Trie, MarisaTrie\n",
    "from genre.fairseq_model import mGENRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/lang_title2wikidataID-normalized_with_redirect.pkl\", \"rb\") as f:\n",
    "    lang_title2wikidataID = pickle.load(f)\n",
    "\n",
    "with open(\"data/titles_lang_all105_marisa_trie_with_redirect.pkl\", \"rb\") as f2:\n",
    "    trie = pickle.load(f2)\n",
    "    \n",
    "model = mGENRE.from_pretrained(\"models/fairseq_multilingual_entity_disambiguation\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f90adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sentences_it.csv\", \"r\") as f:\n",
    "    dict_reader = csv.DictReader(f, delimiter=\",\")\n",
    "    data = list(dict_reader)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb43062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "tagger2 = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "nlp = pipeline(\"ner\", model=tagger2, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb403a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = data[0][\"sentence\"]\n",
    "ner_result = nlp(txt, aggregation_strategy='simple')\n",
    "print(ner_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "\n",
    "pbar = tqdm(total=len(data))\n",
    "for item in data:\n",
    "    entities = list()\n",
    "    text=item[\"sentence\"]\n",
    "    ner_result = nlp(text, aggregation_strategy='simple')\n",
    "    entities = []\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    wb_ids = []\n",
    "    scores = []\n",
    "    for ent in ner_result:\n",
    "        start_pos = ent[\"start\"]\n",
    "        end_pos = ent[\"end\"]\n",
    "        label = ent[\"entity_group\"]\n",
    "        mention = text[0:start_pos]+\" [START] \"+ text[start_pos:end_pos]+ \" [END]\"+text[end_pos:]\n",
    "        start_positions.append(start_pos)\n",
    "        end_positions.append(end_pos)\n",
    "        labels.append(label)\n",
    "        sentences.append(mention)\n",
    "    results = model.sample(\n",
    "        sentences,\n",
    "        prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "            e for e in trie.get(sent.tolist()) if e < len(model.task.target_dictionary)\n",
    "            ],\n",
    "            text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "            marginalize=True,\n",
    "        )\n",
    "    \n",
    "    for result in results:\n",
    "        candidate = result[0]\n",
    "        name = candidate[\"texts\"][0]\n",
    "        score = candidate[\"score\"].item()\n",
    "        wb_id = candidate[\"id\"]\n",
    "        entities.append(name)\n",
    "        scores.append(score)\n",
    "        wb_ids.append(wb_id)\n",
    "    \n",
    "    labels = list(zip(start_positions, end_positions, labels, scores, entities, wb_ids))\n",
    "    for start_pos, end_pos, label, score, alias, wb_ids in labels:\n",
    "        output.append(\n",
    "            {\n",
    "                \"id\":item[\"id\"],\n",
    "                \"start_pos\":start_pos,\n",
    "                \"end_pos\":end_pos,\n",
    "                \"type\":label,\n",
    "                \"alias\":alias,\n",
    "                \"wb_id\":wb_ids,\n",
    "                \"score\":score\n",
    "            }\n",
    "        )\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = output[0].keys()\n",
    "\n",
    "a_file = open(\"output_nel.csv\", \"w\")\n",
    "dict_writer = csv.DictWriter(a_file, keys)\n",
    "dict_writer.writeheader()\n",
    "dict_writer.writerows(output)\n",
    "a_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
