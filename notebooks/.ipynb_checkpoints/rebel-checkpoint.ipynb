{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b37bca-a338-48c9-9c7c-f8ece21c68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import re\n",
    "\n",
    "def extract_triplets_typed(text):\n",
    "    triplets = []\n",
    "    relation = ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    subject, relation, object_, object_type, subject_type = '','','','',''\n",
    "\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").replace(\"tp_XX\", \"\").replace(\"__en__\", \"\").split():\n",
    "        if token == \"<triplet>\" or token == \"<relation>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'head_type': subject_type, 'type': relation.strip(),'tail': object_.strip(), 'tail_type': object_type})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token.startswith(\"<\") and token.endswith(\">\"):\n",
    "            if current == 't' or current == 'o':\n",
    "                current = 's'\n",
    "                if relation != '':\n",
    "                    triplets.append({'head': subject.strip(), 'head_type': subject_type, 'type': relation.strip(),'tail': object_.strip(), 'tail_type': object_type})\n",
    "                object_ = ''\n",
    "                subject_type = token[1:-1]\n",
    "            else:\n",
    "                current = 'o'\n",
    "                object_type = token[1:-1]\n",
    "                relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '' and object_type != '' and subject_type != '':\n",
    "        triplets.append({'head': subject.strip(), 'head_type': subject_type, 'type': relation.strip(),'tail': object_.strip(), 'tail_type': object_type})\n",
    "    return triplets\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/mrebel-large-32\", src_lang=\"en_XX\", tgt_lang=\"tp_XX\") \n",
    "# Here we set English (\"en_XX\") as source language. To change the source language swap the first token of the input for your desired language or change to supported language. For catalan (\"ca_XX\") or greek (\"el_EL\") (not included in mBART pretraining) you need a workaround:\n",
    "# tokenizer._src_lang = \"ca_XX\"\n",
    "# tokenizer.cur_lang_code_id = tokenizer.convert_tokens_to_ids(\"ca_XX\")\n",
    "# tokenizer.set_src_lang_special_tokens(\"ca_XX\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/mrebel-large-32\")\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 256,\n",
    "    \"length_penalty\": 0,\n",
    "    \"num_beams\": 3,\n",
    "    \"num_return_sequences\": 3,\n",
    "    \"forced_bos_token_id\": None,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_pos_from_triplets(triplets, text, idx=None):\n",
    "    triples = []\n",
    "    min_start_head = -1\n",
    "    for triple in triplets:\n",
    "        min_dist_tail = 100000\n",
    "        head = triple[\"head\"]\n",
    "        matches_head = re.finditer(head, text)\n",
    "        matches_head = [(m.start(0), m.end(0)) for m in matches_head]\n",
    "        tail = triple[\"tail\"]\n",
    "        matches_tail = re.finditer(tail, text)\n",
    "        matches_tail = [(m.start(0), m.end(0)) for m in matches_tail]\n",
    "        for x in matches_head:\n",
    "            if x[0]>min_start_head:\n",
    "                min_start_head = x[0]\n",
    "                start_pos_head = x[0]\n",
    "                end_pos_head = start_pos_head+len(head)\n",
    "                break\n",
    "        for y in matches_tail:\n",
    "            dist_tail = abs(y[0]-end_pos_head)\n",
    "            if dist_tail<min_dist_tail:\n",
    "                start_pos_tail = y[0]\n",
    "                min_dist_tail = dist_tail\n",
    "        if idx==None and len(matches_tail)>0 and len(matches_head)>0:\n",
    "            output = {\"head_start\":start_pos_head, \n",
    "                      \"head_end\":start_pos_head+len(head),\n",
    "                      \"head_type\":triple[\"head_type\"],\n",
    "                      \"head\":head,\n",
    "                      \"tail_start\":start_pos_tail,\n",
    "                      \"tail_end\":start_pos_tail+len(tail),\n",
    "                      \"tail_type\":triple[\"tail_type\"],\n",
    "                      \"tail\":tail,\n",
    "                      \"relation\":triple[\"type\"]}\n",
    "            triples.append(output)\n",
    "        elif len(matches_tail)>0 and len(matches_head)>0:\n",
    "            output = {\"text_idx\":idx,\n",
    "                      \"head_start\":start_pos_head, \n",
    "                      \"head_end\":start_pos_head+len(head),\n",
    "                      \"head_type\":triple[\"head_type\"],\n",
    "                      \"head\":head,\n",
    "                      \"tail_start\":start_pos_tail,\n",
    "                      \"tail_end\":start_pos_tail+len(tail),\n",
    "                      \"tail_type\":triple[\"tail_type\"],\n",
    "                      \"tail\":tail,\n",
    "                      \"relation\":triple[\"type\"]}\n",
    "            triples.append(output)\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bff48a9-1742-4286-8d64-a842e406914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"../data/sentences.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = list(csv.DictReader(f=f, delimiter=\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d18c064-3f8c-4db3-a706-423154bbe0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'type_head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m decoded_preds:\n\u001b[0;32m     20\u001b[0m     triplets \u001b[38;5;241m=\u001b[39m extract_triplets_typed(sentence)\n\u001b[1;32m---> 21\u001b[0m     triplets \u001b[38;5;241m=\u001b[39m \u001b[43mget_pos_from_triplets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriplets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, triplet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(triplets):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(triplet)\n",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m, in \u001b[0;36mget_pos_from_triplets\u001b[1;34m(triplets, text, idx)\u001b[0m\n\u001b[0;32m     89\u001b[0m         triples\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matches_tail)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matches_head)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     91\u001b[0m         output \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m:idx,\n\u001b[0;32m     92\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_start\u001b[39m\u001b[38;5;124m\"\u001b[39m:start_pos_head, \n\u001b[0;32m     93\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_end\u001b[39m\u001b[38;5;124m\"\u001b[39m:start_pos_head\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(head),\n\u001b[1;32m---> 94\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_type\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[43mtriple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     95\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m\"\u001b[39m:head,\n\u001b[0;32m     96\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail_start\u001b[39m\u001b[38;5;124m\"\u001b[39m:start_pos_tail,\n\u001b[0;32m     97\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail_end\u001b[39m\u001b[38;5;124m\"\u001b[39m:start_pos_tail\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(tail),\n\u001b[0;32m     98\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail_type\u001b[39m\u001b[38;5;124m\"\u001b[39m:triple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype_tail\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     99\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail\u001b[39m\u001b[38;5;124m\"\u001b[39m:tail,\n\u001b[0;32m    100\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m:triple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m    101\u001b[0m         triples\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m triples\n",
      "\u001b[1;31mKeyError\u001b[0m: 'type_head'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "output = []\n",
    "\n",
    "pbar = tqdm(total=len(data))\n",
    "for sample in data:\n",
    "    triples = []\n",
    "    text_idx = sample[\"id\"]\n",
    "    text = sample[\"sentence\"]\n",
    "    model_inputs = tokenizer(text, max_length=256, padding=True, truncation=True, return_tensors = 'pt')\n",
    "    generated_tokens = model.generate(\n",
    "    model_inputs[\"input_ids\"].to(model.device),\n",
    "    attention_mask=model_inputs[\"attention_mask\"].to(model.device),\n",
    "    decoder_start_token_id = tokenizer.convert_tokens_to_ids(\"tp_XX\"),\n",
    "    **gen_kwargs,\n",
    "    )\n",
    "    # Extract text\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "    for sentence in decoded_preds:\n",
    "        triplets = extract_triplets_typed(sentence)\n",
    "        triplets = get_pos_from_triplets(triplets, text, text_idx)\n",
    "        for idx, triplet in enumerate(triplets):\n",
    "            print(triplet)\n",
    "            if triplet not in triples:\n",
    "                triples.append(triplet)\n",
    "    output.extend(triples)\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "print(output[0])\n",
    "keys = output[0].keys()\n",
    "a_file = open(\"../results/mrebel/output-32.csv\", \"w\", encoding=\"utf-8\")\n",
    "dict_writer = csv.DictWriter(a_file, keys)\n",
    "dict_writer.writeheader()\n",
    "dict_writer.writerows(output)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f682e-ed65-47fb-8e0a-e97f001ce7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
